#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯•é¡¹ç›®å‡çº§åŠŸèƒ½
"""

import sys
import time
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config import get_config, ConfigManager
from utils.error_handler import error_handler, resource_monitor, log_manager
from utils.performance_monitor import performance_monitor, PerformanceAnalyzer
from models.advanced_controller import AdvancedTLSController
from models.advanced_env import AdvancedTrafficEnv

logger = logging.getLogger(__name__)


def test_config_system():
    """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
    print("\nğŸ”§ æµ‹è¯•é…ç½®ç³»ç»Ÿ...")
    
    try:
        # æµ‹è¯•é…ç½®åŠ è½½
        config = get_config()
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   è®­ç»ƒæ­¥æ•°: {config.training.timesteps}")
        print(f"   å­¦ä¹ ç‡: {config.training.learning_rate}")
        print(f"   å¥–åŠ±æƒé‡: {config.get_reward_weights()}")
        
        # æµ‹è¯•é…ç½®æ›´æ–°
        config.update_reward_weights({'waiting_time': 0.4})
        print(f"âœ… é…ç½®æ›´æ–°æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†ç³»ç»Ÿ"""
    print("\nğŸ›¡ï¸ æµ‹è¯•é”™è¯¯å¤„ç†ç³»ç»Ÿ...")
    
    try:
        # æµ‹è¯•é‡è¯•è£…é¥°å™¨
        @error_handler.retry_on_error(max_retries=2, delay=0.1)
        def failing_function():
            raise ValueError("æµ‹è¯•é”™è¯¯")
        
        # æµ‹è¯•å®‰å…¨æ‰§è¡Œ
        result = error_handler.safe_execute(
            lambda: 1 / 0, 
            default_return="å®‰å…¨é»˜è®¤å€¼"
        )
        print(f"âœ… å®‰å…¨æ‰§è¡Œæµ‹è¯•: {result}")
        
        # æµ‹è¯•é”™è¯¯è·Ÿè¸ª
        success = error_handler.track_error("test_error", max_count=5)
        print(f"âœ… é”™è¯¯è·Ÿè¸ªæµ‹è¯•: {success}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_resource_monitoring():
    """æµ‹è¯•èµ„æºç›‘æ§"""
    print("\nğŸ“Š æµ‹è¯•èµ„æºç›‘æ§...")
    
    try:
        # æµ‹è¯•ç³»ç»Ÿä¿¡æ¯è·å–
        system_info = resource_monitor.get_system_info()
        print(f"âœ… ç³»ç»Ÿä¿¡æ¯: {system_info}")
        
        # æµ‹è¯•èµ„æºæ£€æŸ¥
        memory_ok = resource_monitor.check_memory_usage(threshold_mb=4096)
        cpu_ok = resource_monitor.check_cpu_usage(threshold_percent=95)
        disk_ok = resource_monitor.check_disk_space(threshold_gb=0.5)
        
        print(f"âœ… èµ„æºæ£€æŸ¥ - å†…å­˜: {memory_ok}, CPU: {cpu_ok}, ç£ç›˜: {disk_ok}")
        
        return True
        
    except Exception as e:
        print(f"âŒ èµ„æºç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_performance_monitoring():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§"""
    print("\nâš¡ æµ‹è¯•æ€§èƒ½ç›‘æ§...")
    
    try:
        # æ¨¡æ‹Ÿä¸€äº›æ€§èƒ½æ•°æ®
        for i in range(10):
            performance_monitor.update(
                episode=1,
                step=i,
                metrics={
                    'reward': -100 + i * 10,
                    'waiting_time': 50 - i * 2,
                    'queue_length': 10 - i,
                    'throughput': i * 2,
                    'speed': 5 + i * 0.5,
                    'phase_switches': i // 3
                }
            )
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        recent_perf = performance_monitor.collector.get_recent_performance(window=5)
        print(f"âœ… æ€§èƒ½ç»Ÿè®¡: {recent_perf}")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        analyzer = PerformanceAnalyzer(performance_monitor.collector)
        report = analyzer.generate_report()
        print(f"âœ… åˆ†ææŠ¥å‘Šç”ŸæˆæˆåŠŸï¼Œå»ºè®®æ•°: {len(report.get('recommendations', []))}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_controller_improvements():
    """æµ‹è¯•æ§åˆ¶å™¨æ”¹è¿›"""
    print("\nğŸš¦ æµ‹è¯•æ§åˆ¶å™¨æ”¹è¿›...")
    
    try:
        # åˆ›å»ºæ§åˆ¶å™¨ï¼ˆä¸è¿æ¥SUMOï¼‰
        controller = AdvancedTLSController("test_tls")
        print(f"âœ… æ§åˆ¶å™¨åˆ›å»ºæˆåŠŸï¼Œåˆå§‹åŒ–çŠ¶æ€: {controller.is_initialized}")
        
        # æµ‹è¯•å®‰å…¨å½’ä¸€åŒ–
        normalized = controller._safe_normalize(50, 0, 100)
        print(f"âœ… å®‰å…¨å½’ä¸€åŒ–æµ‹è¯•: {normalized}")
        
        # æµ‹è¯•æ— æ•ˆå€¼ä¿®å¤
        fixed = controller._fix_invalid_value(float('nan'))
        print(f"âœ… æ— æ•ˆå€¼ä¿®å¤æµ‹è¯•: {fixed}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ§åˆ¶å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_environment_stability():
    """æµ‹è¯•ç¯å¢ƒç¨³å®šæ€§"""
    print("\nğŸŒ æµ‹è¯•ç¯å¢ƒç¨³å®šæ€§...")
    
    try:
        # æµ‹è¯•ç¯å¢ƒåˆ›å»ºï¼ˆä¸å¯åŠ¨SUMOï¼‰
        config = get_config()
        
        # æ¨¡æ‹Ÿç¯å¢ƒé…ç½®
        env_config = {
            'reward_weights': config.get_reward_weights(),
            'environment': config.get_environment_params()
        }
        
        print(f"âœ… ç¯å¢ƒé…ç½®éªŒè¯æˆåŠŸ")
        print(f"   å¥–åŠ±æƒé‡æ•°é‡: {len(env_config['reward_weights'])}")
        print(f"   ç¯å¢ƒå‚æ•°æ•°é‡: {len(env_config['environment'])}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç¯å¢ƒç¨³å®šæ€§æµ‹è¯•å¤±è´¥: {e}")
        return False


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§ª å¼€å§‹é¡¹ç›®å‡çº§æµ‹è¯•...")
    print("=" * 60)
    
    tests = [
        ("é…ç½®ç³»ç»Ÿ", test_config_system),
        ("é”™è¯¯å¤„ç†", test_error_handling),
        ("èµ„æºç›‘æ§", test_resource_monitoring),
        ("æ€§èƒ½ç›‘æ§", test_performance_monitoring),
        ("æ§åˆ¶å™¨æ”¹è¿›", test_controller_improvements),
        ("ç¯å¢ƒç¨³å®šæ€§", test_environment_stability),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ æµ‹è¯•ç»“æœæ€»ç»“:")
    
    passed = 0
    total = len(results)
    
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
        if success:
            passed += 1
    
    print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é¡¹ç›®å‡çº§æˆåŠŸï¼")
        return True
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")
        return False


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    success = run_all_tests()
    
    if success:
        print("\nğŸ’¡ å‡çº§åŠŸèƒ½è¯´æ˜:")
        print("   â€¢ é…ç½®ç®¡ç†: ç»Ÿä¸€çš„é…ç½®æ–‡ä»¶å’Œå‚æ•°ç®¡ç†")
        print("   â€¢ é”™è¯¯å¤„ç†: è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯æ¢å¤æœºåˆ¶")
        print("   â€¢ èµ„æºç›‘æ§: å®æ—¶ç›‘æ§å†…å­˜ã€CPUå’Œç£ç›˜ä½¿ç”¨")
        print("   â€¢ æ€§èƒ½ç›‘æ§: è®­ç»ƒè¿‡ç¨‹æ€§èƒ½åˆ†æå’Œå»ºè®®")
        print("   â€¢ æ•°æ®éªŒè¯: è¾“å…¥æ•°æ®è¾¹ç•Œæ£€æŸ¥å’Œæ— æ•ˆå€¼å¤„ç†")
        print("   â€¢ æ—¥å¿—ç³»ç»Ÿ: ç»“æ„åŒ–æ—¥å¿—å’Œé”™è¯¯è¿½è¸ª")
        
        print("\nğŸš€ ç°åœ¨å¯ä»¥ä½¿ç”¨å‡çº§åçš„è®­ç»ƒç³»ç»Ÿ:")
        print("   python stable_train.py --scenario competition --timesteps 50000")
        
        sys.exit(0)
    else:
        sys.exit(1)
